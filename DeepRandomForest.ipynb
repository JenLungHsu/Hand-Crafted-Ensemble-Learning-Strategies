{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def dtanh(y):\n",
    "    return 1.0 - y ** 2\n",
    "class MLPClassifier:\n",
    "    def __init__(self, layers, activation='tanh', epochs=20, learning_rate=0.01):\n",
    "        # 第一层layers[0]为输入层, 层数应等于样本特征数\n",
    "        # 最后一层为输出层\n",
    "        # 再算上中间层, 所以len(layers)最小为3\n",
    "        self.epochs = epochs\n",
    "        self.eta = learning_rate\n",
    "        self.layers = [np.zeros(layers[0])]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            # 随机初始化\n",
    "            weight = np.random.random((layers[i + 1], layers[i]))\n",
    "            layer = np.ones(layers[i + 1])\n",
    "            bias = np.random.random(layers[i + 1])\n",
    "            self.weights.append(weight)\n",
    "            self.layers.append(layer)\n",
    "            self.biases.append(bias)\n",
    "        if activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.dactivation = dtanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = dsigmoid\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            # 随机梯度下降\n",
    "            indexes = np.random.permutation(X.shape[0])\n",
    "            for i in range(X.shape[0]):\n",
    "                self.forward(X[indexes[i]])\n",
    "                self.backward(y[indexes[i]])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\t# binary classification\n",
    "        return np.where(self.predict_prob(X) >= 0.5, 1, 0)\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        y = np.empty((X.shape[0], len(self.layers[-1])))\n",
    "        for i in range(X.shape[0]):\n",
    "            self.forward(X[i])\n",
    "            y[i, :] = self.layers[-1]\n",
    "        return y\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.layers[0][:] = inputs\n",
    "        for i in range(len(self.weights)):\n",
    "            self.layers[i + 1] = self.activation(self.weights[i].dot(self.layers[i]) + self.biases[i])\n",
    "\n",
    "    def backward(self, y):\n",
    "        # y 是真实的标签值\n",
    "        y_predict = self.layers[-1] # y_predict即最后一层softmax的输出值\n",
    "        gradient_neurons = y - y_predict  # softmax 的导数\n",
    "        # 从最后一层到第一层进行遍历, 第0层是输入层, 不在遍历范围内\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            gradient_bias = gradient_neurons\n",
    "            # 最后一层无激活函数\n",
    "            if i < len(self.layers):\n",
    "                 gradient_bias *= self.dactivation(self.layers[i])\n",
    "            gradient_weight = gradient_bias.reshape(-1, 1).dot(self.layers[i - 1].reshape(1, -1))  # weight的梯度是个矩阵\n",
    "            gradient_neurons = gradient_bias.dot(self.weights[i - 1])  # 隐层值的梯度\n",
    "            self.weights[i - 1] += self.eta * gradient_weight\n",
    "            self.biases[i - 1] += self.eta * gradient_bias\n",
    "            # self.layers中的内容无需更新\n",
    "\n",
    "class DeepRandomForest:\n",
    "    def __init__(self, n_estimators=10, max_features=None, max_samples=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_samples = max_samples\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Randomly select features and samples\n",
    "            selected_features = np.random.choice(range(n_features), self.max_features, replace=False)\n",
    "            selected_samples = np.random.choice(range(n_samples), self.max_samples, replace=False)\n",
    "\n",
    "            # Create Simple MLP classifier\n",
    "            clf = MLPClassifier(layers = [len(selected_features),200,100,1])\n",
    "            \n",
    "            # Fit on selected features and samples\n",
    "            clf.fit(X[selected_samples][:, selected_features], y[selected_samples])\n",
    "            self.models.append((clf, selected_features))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], len(self.models)))\n",
    "        \n",
    "        for i, (clf, selected_features) in enumerate(self.models):\n",
    "            predictions[:, i] = clf.predict(X[:, selected_features]).flatten()\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def ensemble_predictions(self, X, y_val=None):\n",
    "        ensemble_preds = self.predict(X)\n",
    "        \n",
    "        # Simple averaging of predictions\n",
    "        final_predictions = np.mean(ensemble_preds, axis=1)\n",
    "        \n",
    "        # Weighted averaging based on performance on a validation set\n",
    "        if y_val is not None:\n",
    "            val_accuracy = []\n",
    "            for preds in ensemble_preds.T:\n",
    "                acc = accuracy_score(y_val, preds)\n",
    "                val_accuracy.append(acc)\n",
    "            \n",
    "            weights = np.array(val_accuracy) / sum(val_accuracy)\n",
    "            final_predictions = np.dot(ensemble_preds, weights)\n",
    "        \n",
    "        return np.round(final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_claim\n",
      "0    54844\n",
      "1     3748\n",
      "Name: count, dtype: int64\n",
      "(109688, 87)\n",
      "is_claim\n",
      "0    54844\n",
      "1    54844\n",
      "Name: count, dtype: int64\n",
      "(70200, 86)\n",
      "(17550, 86)\n",
      "(70200,)\n",
      "(17550,)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import data_processing as df\n",
    "\n",
    "def split_data(X, y, train_ratio=0.8, random_seed=42):\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    data_size = len(X)\n",
    "    index = list(range(data_size))\n",
    "    random.shuffle(index)\n",
    "\n",
    "    train_size = int(data_size*train_ratio)\n",
    "    test_size = data_size - train_size\n",
    "\n",
    "    X_train = X.iloc[index[:train_size]]\n",
    "    y_train = y.iloc[index[:train_size]]\n",
    "    X_test = X.iloc[index[train_size:]]\n",
    "    y_test = y.iloc[index[train_size:]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X = df.df.drop(\"is_claim\", axis=1)\n",
    "y = df.df.loc[:, \"is_claim\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "X_tra, X_val, y_tra, y_val = split_data(X_train, y_train)\n",
    "\n",
    "print(X_tra.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(y_tra.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of validation set: 0.4985754985754986\n"
     ]
    }
   ],
   "source": [
    "# Creating Deep Random Forest\n",
    "deep_rf = DeepRandomForest(n_estimators=10, max_features=86, max_samples=20000)\n",
    "deep_rf.fit(X_tra.values, y_tra.values)\n",
    "\n",
    "# Evaluating the ensemble on test data\n",
    "predictions = deep_rf.ensemble_predictions(X_val.values, y_val=y_val.values)\n",
    "accuracy = accuracy_score(y_val.values, predictions)\n",
    "print(f\"Accuracy of validation set: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test set: 0.5002734980399307\n"
     ]
    }
   ],
   "source": [
    "# Creating Deep Random Forest\n",
    "deep_rf = DeepRandomForest(n_estimators=10, max_features=86, max_samples=20000)\n",
    "deep_rf.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Evaluating the ensemble on test data\n",
    "predictions = deep_rf.ensemble_predictions(X_test.values, y_val=y_test.values)\n",
    "accuracy = accuracy_score(y_test.values, predictions)\n",
    "print(f\"Accuracy of test set: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Roy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
